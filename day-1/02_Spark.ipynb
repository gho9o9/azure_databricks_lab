{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627898a1-ba7e-47d0-911c-f94939093028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# はじめに\n",
    "\n",
    "本ノートブックでは **プロビジョニング汎用コンピューティング** をアタッチしてください。\n",
    "学習観点でのわかりやすさのため、本ノートブックでは一部で hive_metastore や DBFS を利用しますが、これらの機能がサーバレスクラスタを通じて利用できないためです。\n",
    "\n",
    "**注意**  \n",
    "hive_metastore や DBFS はいずれもセキュリティ観点で利用が非推奨となっており、現在はそれぞれ後継機能（Unity Catalog、外部ローケーション）が提供されいます。そうした背景もあり、hive_metastore や DBFS はサーバレスコンピューティングではサポートされていません。  \n",
    "よって本ノートブックではプロビジョニング汎用コンピューティングを使用しますが、hive_metastore や DBFS へのオペレーション自体は後継機能（Unity Catalog、外部ローケーション）でも同様であるためそれぞれ後継機能で置き換えることで動作します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32cbdcb8-cd45-45cd-be16-47e06a5f2931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Spark API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d229322-d235-40c0-ac3a-71621c3428aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## このデータはどのように表現されどのように操作するのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5bea9a7-2c5f-4f4c-ab73-8236d1d982dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark では任意の言語（SQL、Python、Scala、R）を通じて任意の Spark データ構造（RDD、DataFrame、Dataset）に対して同じクエリを表現可能です。\n",
    "\n",
    "![クエリ実行エンジン](https://files.training.databricks.com/images/aspwd/spark_sql_query_execution_engine.png)\n",
    "\n",
    "Spark 初期バージョンではデータセットの低レベル表現である RDD を直接操作するコードを記述する必要がありました。\n",
    "\n",
    "![Unified Engine](https://files.training.databricks.com/images/105/unified-engine.png)\n",
    "\n",
    "しかし現在ではより高レベル（ユーザーフレンドリー）な API である DataFrame が主流でパフォーマンス改善もなされています。\n",
    "\n",
    "![RDD vs DataFrames](https://files.training.databricks.com/images/105/rdd-vs-dataframes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc667110-6bb3-4f71-aced-74b0525bc4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## データフレームの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3923d761-69ed-415d-9902-57629f1ba70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "実際にデータフレームを作成してみましょう。\n",
    "ここでは例として CSV データをデータソースとしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03861578-1369-44bd-b685-a630067eba10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_df = spark.read.csv(\"dbfs:/databricks-datasets/COVID/covid-19-data/us-counties.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12231388-9e34-40f2-928d-e8f7989ef244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(covid_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0efd2fb-4466-41dd-91c9-a8124a087bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5681ba43-d028-48a0-b201-03282ff013a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ヘッダ行がデータレコードになっていたり、データ型がすべて String になっていたりと期待した通りのデータフレームが作成できていないようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "210ce9db-0ae7-4a61-9aeb-92ccaedf0eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "スキーマが明示されていない CSV をロードする際にはいくつかのオプションを指定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f43d41-e096-4ae4-99ec-cbed1d064775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_df = spark.read.csv(\"dbfs:/databricks-datasets/COVID/covid-19-data/us-counties.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ff2ffe-bee4-423e-ac69-e23f0067375c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(covid_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1bca3c-e73f-4603-815e-3ef757acc5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e0d785-92f1-40bb-b89a-24ec69d42a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "レコード数はいくつでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d711520-2fc6-488b-a980-deae927649f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "463c1822-e0ee-4cdb-9d62-309b06913362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "クエリをしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505ceee0-6cee-4a2b-b8a3-2439ff5adf9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(covid_df\n",
    " .sort(covid_df[\"date\"].desc()) \n",
    " .filter(covid_df[\"county\"] == \"Los Angeles\") \n",
    " .limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d2e6a1-f750-4b24-af93-bcc9e872ae9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "上記の Spark ジョブはどのようなものになるのでしょうか？  \n",
    "\n",
    "</br><img src=\"../images/spark.1.png\" width=\"600\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f47daf9-1297-4ec7-86c9-7831e46a2ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## トランスフォーメーション と アクション\n",
    "\n",
    "Spark の各種オペレーションは **トランスフォーメーション** と **アクション** に大きく分類されます。\n",
    "\n",
    "それぞれ以下のように説明されます。\n",
    "* トランスフォーメーションは **怠惰(LAZY)** です\n",
    "* アクションは **懸命(EAGER)** です\n",
    "\n",
    "どういうことでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e386425-fe85-41c8-b68d-d66df61c2605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(covid_df\n",
    " .sort(covid_df[\"date\"].desc()) \n",
    " .filter(covid_df[\"county\"] == \"Los Angeles\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e8fa8d-51bf-4675-ade6-ac0153d4022d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "クエリの結果は表示されず実行は即座に完了します。それは **sort** と **filter** は遅延評価される`トランスフォーメーション`であるためです。\n",
    "\n",
    "遅延評価にはいくつかのメリットがあります。\n",
    "* 最初からすべてのデータをロードする必要がありません。\n",
    "  * リソースにも限界がありますし非効率です。\n",
    "* オペレーションの並列化が容易です。\n",
    "  * 単一マシン、単一スレッド、単一のデータ要素に対して、N個の異なるトランスフォーメーションを処理することが可能です。\n",
    "* 最も重要なことは遅延評価により様々な最適化を適用できるようになります。\n",
    "  * 最適化を担う [Spark オプティマイザー（**Catalyst**）](https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)ができることは様々です。\n",
    "  \n",
    "![Catalyst](https://files.training.databricks.com/images/105/catalyst-diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1558b84-39bf-4634-8313-834e18acb93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(covid_df\n",
    " .sort(covid_df[\"date\"].desc()) \n",
    " .filter(covid_df[\"county\"] == \"Los Angeles\") \n",
    " .show()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487b6075-85a1-46bc-a629-4a144ed90dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "結果が返ってきました。すなわち **show** は`アクション`であることがわかります。\n",
    "\n",
    "最適化されたアクションのプランは Spark UI の Spark ジョブに関連づいたクエリーから確認できます。\n",
    "\n",
    "</br><img src=\"../images/spark.2.png\" width=\"600\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1f7b64-86a2-42aa-a508-23698fdb4539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96a76800-592e-46e5-892d-7f7fcbf8d426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Python ではなく SQL でクエリすることも可能です。\n",
    "\n",
    "例として先ほどまで利用しているデータフレームをテーブルとして永続化しそれを SQL からクエリしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a78db1-70f4-4114-983d-e21e86a87fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_df.createOrReplaceTempView(\"covid_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d737924e-5238-41dc-b98e-0336bbd75316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM covid_table LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a32ec2-680d-44b1-bd1f-6a0324e5ac26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "以下は先ほど Python でクエリした内容を SQL で表現したものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964961bb-1026-48f7-9b5c-a12ac96850c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (SELECT * FROM covid_table WHERE county = \"Los Angeles\" ORDER BY date DESC) SELECT `date`,SUM(`cases`) `column_ab2e38fb774`,SUM(`deaths`) `column_ab2e38fb777` FROM q GROUP BY `date`",
       "commandTitle": "可視化 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "date",
             "id": "column_ab2e38fb770"
            },
            "y": [
             {
              "column": "cases",
              "id": "column_ab2e38fb774",
              "transform": "SUM"
             },
             {
              "column": "deaths",
              "id": "column_ab2e38fb777",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_ab2e38fb774": {
             "type": "line",
             "yAxis": 0
            },
            "column_ab2e38fb777": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "1279b95f-9e4f-4611-ba5c-55936489fc55",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 24.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "alias": "column_ab2e38fb774",
           "args": [
            {
             "column": "cases",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_ab2e38fb777",
           "args": [
            {
             "column": "deaths",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM covid_table WHERE county = \"Los Angeles\" ORDER BY date DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992bf0de-a271-4413-b75a-2ed4f9e4a64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "使い慣れた SQL で自由にクエリができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91be57ba-544d-4761-ad86-96221447aa1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT max(cases) AS max_cases, max(deaths) AS max_deaths, county \n",
    "FROM covid_table \n",
    "GROUP BY county \n",
    "ORDER BY max_cases DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2901dc-7ea9-41e1-be6a-ed70c19ecb24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark による簡単な分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f37f134-0e91-4997-a144-49f3ff732ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "covid-19 データセットに加えて census.gov の国勢調査データを関連付けて簡単な分析をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b264bb7-f011-413c-b130-8656704751a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh wget https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv && cp co-est2019-alldata.csv /dbfs/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8109ff45-c7a0-46b6-8a81-bb5b52f48ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "census_df = spark.read.csv(\"dbfs:/tmp/co-est2019-alldata.csv\", header=True, inferSchema=True)\n",
    "display(census_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f1c73af-a053-4820-9817-d9ec458ffbf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "covid-19 内の fipsコード列（連邦情報処理標準における州群コード）に対応する fipsコード列を国勢調査データ内で生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc98ce8-39d0-455f-b5a0-1b1cbc0a5288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def make_fips(state_code, county_code):\n",
    "  if len(str(county_code)) == 1:\n",
    "    return str(state_code) + \"00\" + str(county_code)\n",
    "  elif len(str(county_code)) == 2:\n",
    "    return str(state_code) + \"0\" + str(county_code)\n",
    "  else:\n",
    "    return str(state_code) + str(county_code)\n",
    "\n",
    "make_fips_udf = udf(make_fips, StringType())\n",
    "  \n",
    "census_df = census_df.withColumn(\"fips\", make_fips_udf(census_df.STATE, census_df.COUNTY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99db3bc-a91d-416b-9960-79be7f835afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "両データセットを FIPS コード で INNER JOIN します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9109e57a-092e-49da-80c3-0530bc32c06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "covid_with_census = (covid_df\n",
    "                     .na.drop(subset=[\"fips\"])\n",
    "                     .join(census_df.drop(\"COUNTY\", \"STATE\"), on=['fips'], how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74d0f6f-3f21-4757-a02d-40bfda36ca01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(covid_with_census.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34149c77-8f63-415c-ad40-d86838d172c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "人口の多い郡（人口200万人以上の群）での感染者数の推移を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27173a80-75b1-4fba-8629-8dd45a848520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShjb3ZpZF93aXRoX2NlbnN1cy5maWx0ZXIoIlBPUEVTVElNQVRFMjAxOSA+IDIwMDAwMDAiKS5zZWxlY3QoImNvdW50eSIsICJjYXNlcyIsICJkYXRlIikp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewc50d4ff\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewc50d4ff\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewc50d4ff\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewc50d4ff) SELECT `date`,`county`,SUM(`cases`) `column_86ca006e93` FROM q GROUP BY `county`,`date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewc50d4ff\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "可視化 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "county",
             "id": "column_86ca006e92"
            },
            "x": {
             "column": "date",
             "id": "column_86ca006e91"
            },
            "y": [
             {
              "column": "cases",
              "id": "column_86ca006e93",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_86ca006e93": {
             "name": "cases",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "61f401b0-e959-4254-b7a5-16949a01b2ca",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 35.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "column": "county",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "column": "county",
           "type": "column"
          },
          {
           "alias": "column_86ca006e93",
           "args": [
            {
             "column": "cases",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(covid_with_census.filter(\"POPESTIMATE2019 > 2000000\").select(\"county\", \"cases\", \"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd80a148-27d6-4f82-9f17-5159911303b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "covid-19 データセットは日毎に新しい行が追加されるので、感染者数は日毎に増加します。郡ごとの最新の数のみを取得しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08895634-1916-4043-baa6-4f000b2f0c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, col\n",
    "from pyspark.sql import Window\n",
    "\n",
    "w = Window.partitionBy(\"fips\").orderBy(col(\"date\").desc())\n",
    "current_covid_rates = (covid_with_census\n",
    "                       .withColumn(\"row_num\", row_number().over(w))\n",
    "                       .filter(col(\"row_num\") == 1)\n",
    "                       .drop(\"row_num\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7b32fb-e94d-4443-925c-8495df15c826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(current_covid_rates.limit(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c7a805-fc20-4309-9e10-356cb62b3d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "最も困難に直面した郡はどこでしょうか？\n",
    "ここでは総人口に対する感染者数の割合を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a9a86a-9581-447c-91fb-133b68a6feef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_covid_rates = (current_covid_rates\n",
    "                       .withColumn(\"case_rates_percent\", 100*(col(\"cases\")/col(\"POPESTIMATE2019\")))\n",
    "                       .sort(col(\"case_rates_percent\").desc()))\n",
    "\n",
    "# トップ10の郡を参照します\n",
    "display(current_covid_rates.select(\"county\", \"state\", \"cases\", \"POPESTIMATE2019\", \"case_rates_percent\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56fe6cc-f2ec-4661-a95a-da43f13fd634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Pandas と Pyspark によるデータ処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833b1197-09fb-4b64-a3f3-c2592725e603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pandasとは？\n",
    "\n",
    "データ処理と言えば Pandas だ！\n",
    "\n",
    "Pandas は構造化データとしてのデータの読み込みや読み込んだデータに対するデータ処理（加工）を行うために最も使用されている Python ライブラリの一つです。Pandas ライブラリは、データ分析、機械学習、データサイエンスプロジェクトなどで多く使われています。\n",
    "\n",
    "Pandas は CSV、JSON、SQL などのフォーマットからデータをロードすることができ、SQL テーブルと同じように行と列を含む構造化オブジェクトであるデータフレームを作成します。\n",
    "\n",
    "分散処理をサポートしていないので増大するデータをサポートするために追加の馬力を必要とした際はより強力なマシンを導入する必要があります。また Pandasデータフレームは遅延評価はされず可変です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "859d67d8-942a-4e07-b3f2-fc93e448a3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pandasデータフレームの例\n",
    "PythonでPandasライブラリを使用するためには、import pandas as pd を用いてインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2044a1c6-47fd-45e9-a0bd-8514ccc89e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "data = [[\"James\",\"\",\"Smith\",30,\"M\",60000], \n",
    "        [\"Michael\",\"Rose\",\"\",50,\"M\",70000], \n",
    "        [\"Robert\",\"\",\"Williams\",42,\"\",400000], \n",
    "        [\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000], \n",
    "        [\"Jen\",\"Mary\",\"Brown\",45,None,0]] \n",
    "columns=['First Name','Middle Name','Last Name','Age','Gender','Salary']\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "pandasDF=pd.DataFrame(data=data, columns=columns) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "953ff43a-c868-45f6-ad70-eefbe72e4ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pandas データフレームの関数\n",
    "Pandasデータフレームに実行できる関数のいくつかを以下に示します。デフォルトでそれぞれのカラムに対して関数が適用されます。\n",
    "\n",
    "- df.count() – それぞれのカラムのカウントを返します(カウントには非null値のものだけが含まれます)。\n",
    "- df.corr() – データフレーム内のカラムの相関を返します。\n",
    "- df.head(n) – 上からn行を返します。\n",
    "- df.max() – それぞれのカラムの最大値を返します。\n",
    "- df.mean() – それぞれのカラムの平均値を返します。\n",
    "- df.median() – それぞれのカラムの中央値を返します。\n",
    "- df.min() – それぞれのカラムの最小値を返します。\n",
    "- df.std() – それぞれのカラムの標準偏差を返します。\n",
    "- df.tail(n) – 最後のn行を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9089653-3b00-4520-b26f-63718e463a4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- count() ---\")\n",
    "print(pandasDF.count())\n",
    "print(\"\\n--- max() ---\")\n",
    "print(pandasDF.max())\n",
    "print(\"\\n--- mean() ---\")\n",
    "print(pandasDF.mean())\n",
    "print(\"\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9deaeaf0-80a0-49c6-ba73-2e8d7c11fad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PySparkとは？\n",
    "\n",
    "シングルマシンで実行する Pandas に対して PySpark は複数マシンでの分散処理で実行します。より大きなデータセットを取り扱う場合には PySpark が最適です。\n",
    "\n",
    "Python で Spark 処理を記述する PySpark はデータサイエンス、機械学習コミュニティで広く利用されています。\n",
    "\n",
    "PySpark は Apache Spark の機能を用いて Python を実行するための Pythonで記述されたライブラリです。PySpark を用いることで分散クラスターでアプリケーションを並列に実行することができます。\n",
    "\n",
    "言い換えると Apache Spark は大規模かつパワフルな分散データ処理、機械学習アプリケーションのための分析処理エンジンです。\n",
    "\n",
    "![](https://qiita-user-contents.imgix.net/https%3A%2F%2Fi1.wp.com%2Fsparkbyexamples.com%2Fwp-content%2Fuploads%2F2020%2F08%2FWhat-is-PySpark.png%3Fresize%3D1024%252C164%26ssl%3D1?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&s=ce59994dc84b42a2caddb1d81a7f797c)\n",
    "\n",
    "Spark は基本的に Scala で記述されており、以降業界での導入が進んだことで Py4J を用いて Python 向け API PySpark がリースされました。Py4J は PySpark 内でインテグレーションされており Python が動的に JVM オブジェクトとやりとりすることを可能にしているので PySpark を実行するには Python、Apache Spark と Java をインストールする必要があります。さらに、開発においては PySpark アプリケーションを実行するために Spyder IDE や Jupyter notebook のような有用なツールを数多く備えている(機械学習コミュニティで広く使われている) Anaconda ディストリビューションを使うこともできます。\n",
    "\n",
    "Spark 環境をマネージドサービスとして提供する Databricks はおいてはこうしたセットアップを意識する必要はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65416ac1-d496-4ce1-af75-84f048980270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark の機能\n",
    "- インメモリの計算処理\n",
    "- 並列化による分散処理\n",
    "- さまざまなクラスターマネージャ(Spark、Yarn、Mesosなど)で利用可能\n",
    "- フォールトトレラント\n",
    "- イミュータブル\n",
    "- 遅延評価\n",
    "- キャッシュ & 永続化\n",
    "- データフレームを使う際のビルトインの最適化処理\n",
    "- ANSI SQLのサポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5208cae3-f193-46f5-8908-b8a1ae354352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark の特徴\n",
    "- PySparkは、分散によってデータを効率的に処理できる汎用、インメモリ、分散処理エンジンです。\n",
    "- データ取り込みパイプラインにおいてPySparkを用いることで非常に大きな効果を得ることができます。\n",
    "- PySparkを用いることで、Hadoop HDFS、Azure Storage、AWS S3、Google GCSなど数多くのファイルシステムのデータを処理することができます。\n",
    "- PySparkはストリーミングやKafkaを用いてリアルタイムデータを処理することに使うことができます。\n",
    "- PySparkのストリーミングを用いることで、ファイルシステムやソケットからのストリームからファイルをストリーミングすることができます。\n",
    "- PySparkはネイティブで機械学習、グラフライブラリを持っています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8562dc6-bb71-42d3-87ae-1cdc84cb73f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark 優位性\n",
    "\n",
    "- お使いのデータが膨大かつ毎年増加し続けており、処理時間を改善したい。\n",
    "- フォールトトレラントが必要（分散処理による耐性強化）。\n",
    "- ANSI SQL互換性や使用する言語のバライエティ(Python、Scala、Java、R)。\n",
    "- 外部データとの接続性（Parquet、Avro、Hive、Casandra、Snowflakeなどからデータを読み込みたい）。\n",
    "- データをストリーミングしリアルタイムで処理したい。\n",
    "\n",
    "**従来、分散コンピューティングである Spark 環境のセットアップや運用は大変でしたが、Spark をマネージドサービスとして提供する Databricks はおいてはこうしたセットアップた運用を意識する必要が無くなります。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec341e8-766d-4392-aad0-a343b00f998f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySparkデータフレームの例\n",
    "先ほどの Pandas での記述に相当する PySpark コードがこちらです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf17dfd-1d2b-43ef-bcb2-5f7175ddfe71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",30,\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",50,\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",42,\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",45,\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"Age\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f8a982-45c5-47f2-b480-04a9407792d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, col, max\n",
    "#Example 1\n",
    "df2=pysparkDF.select(mean(\"age\"),mean(\"salary\")) \\\n",
    "             .show()\n",
    "#Example 2\n",
    "pysparkDF.groupBy(\"gender\") \\\n",
    "         .agg(mean(\"age\"),mean(\"salary\"),max(\"salary\")) \\\n",
    "         .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11641388-d3ff-4646-985f-c6c68b4ecb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pandas API on Spark\n",
    "\n",
    "とはいえ、データ処理と言えば Pandas だ！そんなエンジニアのために Pandas API を利用しつつ Spark 処理できる API として **Pandas API on Spark**(旧 Koalas) があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b534839-525c-4428-bd0a-82ad479c6f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "pandas_api() によって Spark データフレームを Pandas API でデータ処理できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cb3fca-4940-480a-b018-8499525229fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "psdf = current_covid_rates.pandas_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9671ffca-697d-44cb-b12c-7651795849e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "pandas_api() による変換後は Pandas 作法でデータ処理を記述できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af23980-11eb-4813-aeee-46eda7257282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6521151-67fa-4d66-a45a-9302125fea32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbe5503-b36e-49a1-8a0f-fd8e90df8077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169dc865-79bd-4b60-b6e7-760ba77d0abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "psdf.sort_values(by='deaths', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e1af05d-8bb9-449d-9c8b-343a83146799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## データフレームの相互変換\n",
    "\n",
    "もしくは、それぞれのデータフレームを相互変換することも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5045d80-911e-4f2f-9ee6-c64e1d6bd546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pandas データフレーム から Spark データフレーム の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7274769e-6330-4d90-a7d5-f115313011d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create PySpark DataFrame from Pandas\n",
    "pysparkDF = spark.createDataFrame(pandasDF) \n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35214398-8bdf-4180-906f-1ea97cdfda47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark データフレーム から Pandas データフレーム の作成\n",
    "\n",
    "**留意**：toPandas()メソッドは、データをSparkドライバーのメモリーに集めるアクションなので、大規模データセットを取り扱っている際には注意を払う必要があります。収集したデータがSparkドライバーのメモリーに収まらない際にはOutOfMemoryExceptionに遭遇することになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c697642-0438-4800-90e7-375b58d8165d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Convert PySpark to Pandas\n",
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d2defc-641e-4fbf-b141-74999599e1d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 参考\n",
    "- [今さら聞けないPython - Sparkのご紹介](https://qiita.com/taka_yayoi/items/5415db284b96c7c60d44)\n",
    "- [Pandas vs PySpark DataFrame With Examples](https://sparkbyexamples.com/pyspark/pandas-vs-pyspark-dataframe-with-examples/amp/)\n",
    "- [Spark 上の Pandas API](https://learn.microsoft.com/ja-jp/azure/databricks/pandas/pandas-on-spark)]\n",
    "- [PySpark と pandas DataFrame 間で変換する](https://learn.microsoft.com/ja-jp/azure/databricks/pandas/pyspark-pandas-conversion)\n",
    "- [pandasユーザーがPandas API on Sparkでつまづいたあれこれ](https://kakehashi-dev.hatenablog.com/entry/2022/12/24/090000)\n",
    "- [Ray on Azure Databricks とは](https://learn.microsoft.com/ja-jp/azure/databricks/machine-learning/ray/)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8338176362084423,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
